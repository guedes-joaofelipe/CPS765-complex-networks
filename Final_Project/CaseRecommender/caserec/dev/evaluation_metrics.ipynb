{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "ranking = np.array([1, 0, 0, 0, 0, 0])\n",
    "rankings = [ranking, ranking, [0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "def reciprocal_rank(ranking):\n",
    "    index_found_ranks = np.where(ranking == 1)[0]\n",
    "    if index_found_ranks.size > 0:\n",
    "        rank = index_found_ranks[0] + 1\n",
    "        return 1/float(rank)    \n",
    "    return 0\n",
    "\n",
    "def mean_reciprocal_rank(rankings):\n",
    "    return np.mean([reciprocal_rank(ranking) for ranking in rankings])\n",
    "\n",
    "def ndcg_at_k(ranking):\n",
    "    \"\"\"\n",
    "    Score is normalized discounted cumulative gain (ndcg). Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "\n",
    "    :param ranking: ranking to evaluate in dcg format [0, 0, 1], where 1 is correct info\n",
    "    :type ranking: list\n",
    "\n",
    "    :return: Normalized discounted cumulative gain\n",
    "    :rtype: float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ranking = np.asfarray(ranking)\n",
    "    r_ideal = np.asfarray(sorted(ranking, reverse=True))\n",
    "    dcg_ideal = r_ideal[0] + np.sum(r_ideal[1:] / np.log2(np.arange(2, r_ideal.size + 1)))\n",
    "    dcg_ranking = ranking[0] + np.sum(ranking[1:] / np.log2(np.arange(2, ranking.size + 1)))\n",
    "\n",
    "    return dcg_ranking / dcg_ideal\n",
    "\n",
    "print (ndcg_at_k(ranking))\n",
    "print (reciprocal_rank(ranking))\n",
    "print (mean_reciprocal_rank(rankings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def rank_accuracy(ranking):\n",
    "    \"\"\"\n",
    "    Score is rank accuracy . Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "\n",
    "    For a N-size list\n",
    "    If item is outside the sequence: 0 score\n",
    "    Elif item in sequence but outside the right position: = 1/N score\n",
    "    Else (item in the sequence and in the right position): = 1 score\n",
    "\n",
    "    \"\"\"    \n",
    "    ranking = np.asfarray(ranking)\n",
    "    ranking_ideal = np.asfarray(sorted(ranking, reverse=True))    \n",
    "    ideal_index = np.argsort(-ranking_ideal)\n",
    "    current_index = np.argsort(-ranking)\n",
    "\n",
    "    score = 0\n",
    "    ranking_length = len(ranking)\n",
    "\n",
    "    for item_index in np.arange(ranking_length):\n",
    "        if ranking[item_index] == 0:\n",
    "            item_score = 0\n",
    "        elif item_index == ideal_index[item_index]:\n",
    "            item_score = 1\n",
    "        else:\n",
    "            item_score = float(1/ranking_length)        \n",
    "        score += item_score\n",
    "\n",
    "    return float(score/ranking_length)\n",
    "    \n",
    "def mean_rank_accuracy(rankings):\n",
    "    \"\"\"\n",
    "        Score is the mean rank accuracy of a list of rankings. Relevance is positive real values.  Can use binary\n",
    "        as the previous methods.\n",
    "        \n",
    "        :param rankings: a list of rankings\n",
    "        :ptype: [list, np.array]\n",
    "        \n",
    "        :return: mean rank accuracy\n",
    "        :rtype: float\n",
    "    \"\"\"\n",
    "    return np.mean([rank_accuracy(ranking) for ranking in rankings])\n",
    "\n",
    "ranking = [0, 0.1, 0, 0, 0, 0]\n",
    "rank_accuracy(ranking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current ranking:  [0.  0.1 0.  0.  0.  0. ]\n",
      "Ideal ranking:  [0.1 0.  0.  0.  0.  0. ]\n",
      "Score item 0 (0.0): 0\n",
      "Score item 1 (0.1): 1\n",
      "Score item 2 (0.0): 0\n",
      "Score item 3 (0.0): 0\n",
      "Score item 4 (0.0): 0\n",
      "Score item 5 (0.0): 0\n",
      "Final score:  0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "# ranking = np.asfarray([0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n",
    "ranking = np.asfarray([0, 0.1, 0, 0, 0, 0])\n",
    "r_ideal = np.asfarray(sorted(ranking, reverse=True))\n",
    "print (\"Current ranking: \", ranking)\n",
    "print (\"Ideal ranking: \", r_ideal)\n",
    "ideal_index = np.argsort(-r_ideal)\n",
    "current_index = np.argsort(-ranking)\n",
    "\n",
    "score = 0\n",
    "ranking_length = len(ranking)\n",
    "\n",
    "for item_index in np.arange(ranking_length):\n",
    "    if ranking[item_index] == 0:\n",
    "        item_score = 0\n",
    "    elif item_index == ideal_index[item_index]:\n",
    "        item_score = 1\n",
    "    else:\n",
    "        item_score = float(1/ranking_length)\n",
    "    print (\"Score item {} ({}): {}\".format(item_index, ranking[item_index], item_score))\n",
    "    score += item_score\n",
    "    \n",
    "score = float(score/ranking_length)\n",
    "print (\"Final score: \", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
